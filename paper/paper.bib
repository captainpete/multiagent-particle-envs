@misc{lillicrap2019continuous,
  title={Continuous control with deep reinforcement learning}, 
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  year={2019},
  eprint={1509.02971},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@phdthesis{watkins,
  title={Learning from Delayed Rewards},
  author={Christopher J. C. H. Watkins},
  year={1989},
  school={Kings College},
  url={http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf}
}

@article{2015Natur.518..529M,
  author = {{Mnih}, Volodymyr and {Kavukcuoglu}, Koray and {Silver}, David and {Rusu}, Andrei A. and {Veness}, Joel and {Bellemare}, Marc G. and {Graves}, Alex and {Riedmiller}, Martin and {Fidjeland}, Andreas K. and {Ostrovski}, Georg and {Petersen}, Stig and {Beattie}, Charles and {Sadik}, Amir and {Antonoglou}, Ioannis and {King}, Helen and {Kumaran}, Dharshan and {Wierstra}, Daan and {Legg}, Shane and {Hassabis}, Demis},
  title = "{Human-level control through deep reinforcement learning}",
  journal = {Nature},
  year = 2015,
  month = feb,
  volume = {518},
  number = {7540},
  pages = {529-533},
  doi = {10.1038/nature14236},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2015Natur.518..529M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{SuttonBarto,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  title = {Reinforcement Learning: An Introduction},
  year = {2018},
  isbn = {0262039249},
  publisher = {A Bradford Book},
  address = {Cambridge, MA, USA}
}

@inproceedings{Williams92simplestatistical,
  author = {Ronald J. Williams},
  title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  booktitle = {Machine Learning},
  year = {1992},
  pages = {229--256}
}

@article{bhatnagar2009natural,
  title={Natural actor--critic algorithms},
  author={Bhatnagar, Shalabh and Sutton, Richard S and Ghavamzadeh, Mohammad and Lee, Mark},
  journal={Automatica},
  volume={45},
  number={11},
  pages={2471--2482},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{pmlr-v32-silver14,
  title={Deterministic Policy Gradient Algorithms},
  author={David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
  booktitle={Proceedings of the 31st International Conference on Machine Learning},
  pages={387--395},
  year={2014},
  editor={Eric P. Xing and Tony Jebara},
  volume={32},
  series={Proceedings of Machine Learning Research},
  address={Bejing, China},
  month={22--24 Jun},
  publisher={PMLR},
  pdf={http://proceedings.mlr.press/v32/silver14.pdf},
  url={http://proceedings.mlr.press/v32/silver14.html}
}

@misc{lowe2020multiagent,
  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  year={2020},
  eprint={1706.02275},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{leike2017ai,
  title={AI Safety Gridworlds},
  author={Jan Leike and Miljan Martic and Victoria Krakovna and Pedro A. Ortega and Tom Everitt and Andrew Lefrancq and Laurent Orseau and Shane Legg},
  year={2017},
  eprint={1711.09883},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@article{ghavamzadeh2016bayesian,
  title={Bayesian reinforcement learning: A survey},
  author={Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  journal={arXiv preprint arXiv:1609.04436},
  year={2016}
}

@inproceedings{10.5555/3091125.3091194,
  author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  title = {Multi-Agent Reinforcement Learning in Sequential Social Dilemmas},
  year = {2017},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  pages = {464–473},
  keywords = {agent-based social simulation, social dilemmas, non-cooperative games, markov games, cooperation},
  location = {S\~{a}o Paulo, Brazil},
  series = {AAMAS '17}
}

@article {Silver1140,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/362/6419/1140},
	eprint = {https://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}

@misc{badia2020agent57,
  title={Agent57: Outperforming the Atari Human Benchmark},
  author={Adrià Puigdomènech Badia and Bilal Piot and Steven Kapturowski and Pablo Sprechmann and Alex Vitvitskyi and Daniel Guo and Charles Blundell},
  year={2020},
  eprint={2003.13350},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojtek and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}

@article{barth2018distributed,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Tb, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@inproceedings{chen2018auto,
  title={Auto: Scaling deep reinforcement learning for datacenter-scale automatic traffic optimization},
  author={Chen, Li and Lingys, Justinas and Chen, Kai and Liu, Feng},
  booktitle={Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
  pages={191--205},
  year={2018}
}

@incollection{mao2019learning,
  title={Learning scheduling algorithms for data processing clusters},
  author={Mao, Hongzi and Schwarzkopf, Malte and Venkatakrishnan, Shaileshh Bojja and Meng, Zili and Alizadeh, Mohammad},
  booktitle={Proceedings of the ACM Special Interest Group on Data Communication},
  pages={270--288},
  year={2019}
}

@inproceedings{valadarsky2017learning,
  title={Learning to route},
  author={Valadarsky, Asaf and Schapira, Michael and Shahaf, Dafna and Tamar, Aviv},
  booktitle={Proceedings of the 16th ACM workshop on hot topics in networks},
  pages={185--191},
  year={2017}
}

@misc{dulacarnold2019challenges,
  title={Challenges of Real-World Reinforcement Learning},
  author={Gabriel Dulac-Arnold and Daniel Mankowitz and Todd Hester},
  year={2019},
  eprint={1904.12901},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{abbeel2006application,
  title={An application of reinforcement learning to aerobatic helicopter flight},
  author={Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={1--8},
  year={2006}
}

@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@misc{hoffman2020acme,
  title={Acme: A Research Framework for Distributed Reinforcement Learning},
  author={Matt Hoffman and Bobak Shahriari and John Aslanides and Gabriel Barth-Maron and Feryal Behbahani and Tamara Norman and Abbas Abdolmaleki and Albin Cassirer and Fan Yang and Kate Baumli and Sarah Henderson and Alex Novikov and Sergio Gómez Colmenarejo and Serkan Cabi and Caglar Gulcehre and Tom Le Paine and Andrew Cowie and Ziyu Wang and Bilal Piot and Nando de Freitas},
  year={2020},
  eprint={2006.00979},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@inproceedings{jakobi1995noise,
  title={Noise and the reality gap: The use of simulation in evolutionary robotics},
  author={Jakobi, Nick and Husbands, Phil and Harvey, Inman},
  booktitle={European Conference on Artificial Life},
  pages={704--720},
  year={1995},
  organization={Springer}
}

